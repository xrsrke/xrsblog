{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a848910b-5bca-4bde-925c-457562bbf1e0",
   "metadata": {},
   "source": [
    "---\n",
    "title: AI Alignment\n",
    "description: AI alignment is about making sure that AI systems work in line with what humans want, so that we can all benefit from it. As AI becomes more sophisticated, it is essential to ensure that it is working towards a shared and beneficial future for everyone.\n",
    "date: 2023-02-30\n",
    "toc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025be6b4-c78b-4adb-b690-fae0f839110a",
   "metadata": {},
   "source": [
    "### Human in the loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a7c0a1-f0ed-4a12-b9b5-5ec0d285abcb",
   "metadata": {},
   "source": [
    "Reinforcement Learning from Human Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256596c1-bf7d-4def-bf12-e8532623eb55",
   "metadata": {},
   "source": [
    "![RLHF](./images/openai-rlhf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2688f238-3dad-4b0e-a701-cf49605dd8a2",
   "metadata": {},
   "source": [
    "The loss function for the reward model\n",
    "\n",
    "$\\operatorname{loss}(\\theta)=-\\frac{1}{\\left(\\begin{array}{c}K \\\\ 2\\end{array}\\right)} E_{\\left(x, y_w, y_l\\right) \\sim D}\\left[\\log \\left(\\sigma\\left(r_\\theta\\left(x, y_w\\right)-r_\\theta\\left(x, y_l\\right)\\right)\\right)\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30786e6-cd1e-4caa-aba1-49d8611bbe8c",
   "metadata": {},
   "source": [
    "$y_w$ is the human-prefer label, and $y_l$ is the non-human-prefer label. The goal is make the reward model $r_\\theta$ assigns a higher score for $y_w$ than $y_l$. So we want to turn this score into a probability distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050c233e-0e0b-4c61-9120-1b2e52d1d32c",
   "metadata": {},
   "source": [
    "The objective function for the RL-based model\n",
    "\n",
    "$\\begin{aligned} \\operatorname{objective}(\\phi)= & E_{(x, y) \\sim D_{\\pi_\\phi^{\\mathrm{RL}}}}\\left[r_\\theta(x, y)-\\beta \\log \\left(\\pi_\\phi^{\\mathrm{RL}}(y \\mid x) / \\pi^{\\mathrm{SFT}}(y \\mid x)\\right)\\right]+ \\\\ & \\gamma E_{x \\sim D_{\\text {pretrain }}}\\left[\\log \\left(\\pi_\\phi^{\\mathrm{RL}}(x)\\right)\\right]\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626d76f5-7b3b-48e9-85b4-b738fcc76738",
   "metadata": {},
   "source": [
    "### Red team"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724f3583-b663-4065-98ba-a87fac5865e9",
   "metadata": {},
   "source": [
    "### Sandwitching Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2df1f3-ff98-4182-a05d-891d0a9c2a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
