{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17dbe3b1-437a-46c6-8588-0de76faf0eeb",
   "metadata": {},
   "source": [
    "The loss function described in the text is used to train a Transformer-based neural network for protein generation. The goal of the model is to generate a sequence of amino acids, which together specify a protein, while being controlled by a set of descriptors known as control tags.\n",
    "\n",
    "The variable $a$ represents a sequence of amino acids and has length $n_a-1$. The variable $c$ represents the control tags, which are a set of descriptors such as protein family or source organism, and has length $n_c$. The variable $x$ is formed by prepending the control tag sequence to the amino acid sequence and has length $n=n_a+n_c$. The probability over such a combined sequence is represented by $P(x)$.\n",
    "\n",
    "The language modeling problem is decomposed into a next-token prediction problem, where a token can either be an amino acid or a control tag. The neural network has parameters $\\theta$ and is trained to minimize the negative log-likelihood over a dataset $D={x^1, \\ldots, x^{|D|}}$. The loss function is calculated as follows:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "−\n",
    "1\n",
    "∣\n",
    "�\n",
    "∣\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "∣\n",
    "�\n",
    "∣\n",
    "1\n",
    "�\n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "�\n",
    "log\n",
    "⁡\n",
    "�\n",
    "�\n",
    "(\n",
    "�\n",
    "�\n",
    "�\n",
    "∣\n",
    "�\n",
    "<\n",
    "�\n",
    "�\n",
    ")\n",
    "L(D)=− \n",
    "∣D∣\n",
    "1\n",
    "​\n",
    "  \n",
    "k=1\n",
    "∑\n",
    "∣D∣\n",
    "​\n",
    "  \n",
    "n \n",
    "k\n",
    " \n",
    "1\n",
    "​\n",
    "  \n",
    "i=1\n",
    "∑\n",
    "n \n",
    "k\n",
    " \n",
    "​\n",
    " logp \n",
    "θ\n",
    "​\n",
    " (x \n",
    "i\n",
    "k\n",
    "​\n",
    " ∣x \n",
    "<i\n",
    "k\n",
    "​\n",
    " )\n",
    "Once the neural network is trained, it can generate a new protein $\\underline{a}$ of length $m_a$ controlled by a control tag sequence $\\underline{c}$ of length $m_c$. The generation process starts by sequentially sampling its constituent tokens based on the learned parameters $\\theta$. This is done by calculating the probabilities $p_\\theta\\left(a_1 \\mid \\underline{c}\\right), p_\\theta\\left(a_2 \\mid a_1, \\underline{c}\\right), \\ldots, p_\\theta\\left(a_j \\mid a_{<j}, \\underline{c}\\right)$. The generation process continues until the model generates an 'end of sequence' token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888bcf4c-8381-426f-9d0b-3dfb52e9c3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
