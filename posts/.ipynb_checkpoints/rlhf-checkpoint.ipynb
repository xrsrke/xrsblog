{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1de149ea-f255-4b6b-aadb-cc36781bc16b",
   "metadata": {},
   "source": [
    "---\n",
    "title: How RLHF works?\n",
    "description: | \n",
    "  A summary of the best practices for summarizing output of reproducible scientific documents.\n",
    "date: 2023-02-30\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e55ae5-824d-4e24-ae33-63da9022be7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4c91dc-423d-4b17-9b1a-d9664244e7d5",
   "metadata": {},
   "source": [
    "### How RLHF works?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e55b93-a07b-48d7-97c2-b02e2eeebd9c",
   "metadata": {},
   "source": [
    "Paper: [Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2204.05862)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc8fb12-f001-45dd-a13e-a2572b82baa5",
   "metadata": {},
   "source": [
    "#| column: screen-inset-shaded\n",
    "\n",
    "![openai-rlhf.png](./images/openai-rlhf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eb6e18-b67b-4003-8e25-82970198707d",
   "metadata": {},
   "source": [
    "Outline\n",
    "- Why ChatGPT take does world by storm?\n",
    "    + Train on million of text\n",
    "    + Not align with the human intention\n",
    "        + LM was trained on milition of articles from the internet. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb25b55-0418-45fe-a28d-5081fb1467a0",
   "metadata": {},
   "source": [
    "### Explaining RLHF for a kid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f79c800-adf5-4c84-824d-fc670e52ccbc",
   "metadata": {},
   "source": [
    "### Breaking down RLHF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4f85db-b5da-4815-b9b2-ee089f30eca8",
   "metadata": {},
   "source": [
    "|  | What does it represent | \n",
    "|---------|:-----|\n",
    "| 12      | 12   |\n",
    "| 123     | 123  |\n",
    "| 1       | 1    |\n",
    "\n",
    ": Notations"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e20a8707-bc54-4458-a9ca-4395e0e9ff86",
   "metadata": {},
   "source": [
    "```{python}\n",
    "#| label: tbl-planets\n",
    "#| tbl-cap: Planets\n",
    "\n",
    "from IPython.display import Markdown\n",
    "from tabulate import tabulate\n",
    "table = [[\"Sun\",696000,1989100000],\n",
    "         [\"Earth\",6371,5973.6],\n",
    "         [\"Moon\",1737,73.5],\n",
    "         [\"Mars\",3390,641.85]]\n",
    "Markdown(tabulate(\n",
    "  table, \n",
    "  headers=[\"Planet\",\"R (km)\", \"mass (x 10^29 kg)\"]\n",
    "))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acf6979-80fa-4407-9009-c9049864ab91",
   "metadata": {},
   "source": [
    "output\n",
    "\n",
    "- Align with human preferences\n",
    "- Tweak the training loop of GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d79af52-e3ec-4d74-ab73-17b0dc66a311",
   "metadata": {},
   "source": [
    "- Supervise Fine-tuning (SFT): the model is used to fined tune with prompts and desired output by labelers\n",
    "- Reward Model (RM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa3b4a2-e0d4-4039-8687-4cade8341e63",
   "metadata": {},
   "source": [
    "#### Start with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a99b79-eec8-4490-b7cb-275be45f5ca5",
   "metadata": {},
   "source": [
    "- A pretrained language model\n",
    "- A distribution of prompts on which we want our model to produce aligned outputs\n",
    "- A team of trained human labelers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e371e28-8be4-4cee-a9cd-5f2b39725f4a",
   "metadata": {},
   "source": [
    "#### Steps\n",
    "- Compare output of difference language model on the dataset\n",
    "- Labeler scores each output from these model\n",
    "- Reward model learn to predicts which model output that labelers would prefer\n",
    "- Use reward model as reward function\n",
    "- Fine-tune pre-trained model to maximize the reward using PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cdfec9-bf93-410e-90dd-8d50fedb8da3",
   "metadata": {},
   "source": [
    "### Dateset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce424a30-5c11-4d9b-a1b4-b120719294c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a24a14b6-9f67-445c-b9d6-81f66f8a66a5",
   "metadata": {},
   "source": [
    "### The idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bed6670-e779-4537-8686-0c9b21511d6d",
   "metadata": {},
   "source": [
    "Align with human preferences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bf8fb0-712e-4113-80f5-b54ddb6574bc",
   "metadata": {},
   "source": [
    "- Reward Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0a74a4-afc8-49a7-97f2-be32205e22bd",
   "metadata": {},
   "source": [
    "**Notations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434b7027-68dc-43e6-91d0-c32f53f0d1b5",
   "metadata": {},
   "source": [
    "### Training the reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f1f1d3-7eda-4822-9a0e-11c9afd98c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration CarperAI--openai_summarize_comparisons-79d2c222a15dc8fb\n",
      "Found cached dataset parquet (/Users/education/.cache/huggingface/datasets/CarperAI___parquet/CarperAI--openai_summarize_comparisons-79d2c222a15dc8fb/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"CarperAI/openai_summarize_comparisons\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88852c0-93e3-434a-866c-8a3fffbca5e1",
   "metadata": {},
   "source": [
    "$\\operatorname{loss}(\\theta)=-\\frac{1}{\\left(\\begin{array}{c}\n",
    "K \\\\\n",
    "2\n",
    "\\end{array}\\right)} E_{\\left(x, y_w, y_l\\right) \\sim D}\\left[\\log \\left(\\sigma\\left(r_\\theta\\left(x, y_w\\right)-r_\\theta\\left(x, y_l\\right)\\right)\\right)\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf64669-5788-4548-8267-e01f9e0e857c",
   "metadata": {},
   "source": [
    "::: {.column-margin}\n",
    "Page 8. Formula 1\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b7ae7c-b6ac-4bee-8a80-38e1ef779b9a",
   "metadata": {},
   "source": [
    "```python\n",
    "class PairwiseLoss(nn.Module):\n",
    "    def forward(\n",
    "        self,\n",
    "        chosen_rewards: TensorType[\"batch_size\", 1],\n",
    "        rejected_rewards: TensorType[\"batch_size\", 1]\n",
    "    ) -> TensorType[1]: # A scalar loss\n",
    "        assert len(chosen_rewards) == len(rejected_rewards)\n",
    "        batch_size = len(chosen_rewards)\n",
    "        \n",
    "        # maps the difference between the rewards to a probability\n",
    "        probs = torch.sigmoid(chosen_rewards - rejected_rewards)\n",
    "        return -probs.mean() / batch_size\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fd6c5c-cce8-4201-9c45-d54171d1bc5b",
   "metadata": {},
   "source": [
    "::: {.column-margin}\n",
    "[reward.py](https://github.com/xrsrke/instructGOOSE/blob/2fca05409bd202ff991b829e500b9b3de1d00ca4/instruct_goose/reward.py#L52)\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b45fd7-226b-45f4-b5f7-68b7c049e94a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9090f9fd-8f4e-4aa1-9343-614177339bc7",
   "metadata": {},
   "source": [
    "$y_w$, and $y_l$: represent the output that prefered and non-prefereed by human labeller respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd893da-553e-4cec-aded-ab4a5d1b3087",
   "metadata": {},
   "source": [
    "The goal of the reward model is make it algin with human preference as much as possible. So we want a loss score that indicate if the loss is high that mean the reward model isn't doing good its job and vice versa.\n",
    "- $r_\\theta\\left(x, y_w\\right)$ is the reward scalar of the reward model for prompt $x$ and the summary $y_w$\n",
    "- $r_\\theta\\left(x, y_l\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccd42f8-6fe3-460d-b4a6-239f89173143",
   "metadata": {},
   "source": [
    "##### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67e58fc-e31a-48be-bb69-d59efd122521",
   "metadata": {},
   "source": [
    "present labelers with multiple responses to a prompt, and ask them to rank the responses\n",
    "\n",
    "https://huggingface.co/datasets/openai/webgpt_comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3698625b-4f22-4536-86c3-6ffe0ca0bb03",
   "metadata": {},
   "source": [
    "##### Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cf0ff6-1add-4681-b9e1-8d3147e4c4f8",
   "metadata": {},
   "source": [
    "10 responses. compare pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781c08ea-0709-41df-ac65-973d75a3a530",
   "metadata": {},
   "source": [
    "### SFT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a36659-066d-40fd-b62e-235e5283d0a4",
   "metadata": {},
   "source": [
    "1. https://huggingface.co/Dahoas/gpt2-sft-static\n",
    "2. https://huggingface.co/Dahoas/gptneo-sft-static\n",
    "3. https://huggingface.co/Dahoas/gptj-sft-static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63813e7-975a-4486-9afb-818143584cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c58b576-05df-40c2-908f-82ebd492e242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed418eda-a33f-4749-8bd3-ebcbf85670d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f96d5e-e730-4de6-bcab-ca189436c9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f30723-3a16-4a7e-bf3f-73d8a62df321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c40976-ae74-4a05-94a7-29dd996a0b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.tensor(-0.1)\n",
    "x2 = torch.tensor(0.3)\n",
    "x3 = torch.tensor(0.5)\n",
    "x4 = torch.tensor(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897d699f-4b82-4456-8c6a-fc049c1dea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor(0.3)\n",
    "\n",
    "\n",
    "y2 = torch.tensor(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98432d4-4474-484e-a295-7f65716f9283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5498)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eef6ca-3ce3-4e7d-9652-12c74cf236be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.5544)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.sigmoid(y).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b06fd68-be18-4b4a-94cc-44a3c61aee9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.3412)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.sigmoid(y2).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285757b5-241e-4f65-8661-312ccda2d80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximize log likelihood = minimize negative log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcda73a-2ed0-4ff3-bbd4-fc567355e4bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc04a1d-c05c-4cbc-b9b3-dda2047f4147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4750), tensor(0.5744), tensor(0.6225), tensor(0.6900))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.sigmoid(x1), F.sigmoid(x2), F.sigmoid(x3), F.sigmoid(x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e1c8bd-de26-465f-a2a6-44deca16f8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(1).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10bcd3b-3205-46e2-89ac-454cc95def97",
   "metadata": {},
   "source": [
    "### Objective Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0549ecd-1284-4e2f-a6d8-47c04b448e0d",
   "metadata": {},
   "source": [
    "$\\begin{aligned}\\text { objective }(\\phi)= & E_{(x, y) \\sim D_{\\pi_\\phi^{\\mathrm{RL}}}}\\left[r_\\theta(x, y)-\\beta \\log \\left(\\pi_\\phi^{\\mathrm{RL}}(y \\mid x) / \\pi^{\\mathrm{SFT}}(y \\mid x)\\right)\\right]+ \\\\& \\gamma E_{x \\sim D_{\\text {pretrain }}}\\left[\\log \\left(\\pi_\\phi^{\\mathrm{RL}}(x)\\right)\\right]\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7450f83-a6fb-45a6-b4bd-a603c945745d",
   "metadata": {},
   "source": [
    "```python\n",
    "class AgentObjective(nn.Module):\n",
    "    \"\"\"Agent objective.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Callable, # the language model\n",
    "        sft_model: Callable, # the reference model\n",
    "        reward_model: Callable, # the reward model\n",
    "        gamma: float,\n",
    "        beta: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.sft_model = sft_model\n",
    "        self.reward_model = reward_model\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: TensorType[\"batch_size\", \"seq_len\"],\n",
    "        attention_mask: TensorType[\"batch_size\", \"seq_len\"]\n",
    "    ) -> TensorType[1]: # A scalar objective value\n",
    "        \"\"\"Calculate the objective value given the input ids and attention mask.\"\"\"\n",
    "        model_logits = self.model(input_ids, attention_mask)\n",
    "        model_dist = F.softmax(model_logits, dim=-1)\n",
    "        \n",
    "        sft_logits = self.sft_model(input_ids, attention_mask)\n",
    "        sft_dist = F.softmax(sft_logits, dim=-1)\n",
    "        \n",
    "        reward_score = self.reward_model(input_ids, attention_mask)\n",
    "        ratio = torch.log(model_dist / sft_dist)\n",
    "        \n",
    "        # compute the coherent of the generated text\n",
    "        coherent = torch.log(model_dist)\n",
    "        objective = (reward_score - self.beta*ratio).mean() + self.gamma * coherent.mean()\n",
    "        return objective\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cd9d30-42bc-482d-ab07-bad490a950fc",
   "metadata": {},
   "source": [
    "::: {.column-margin}\n",
    "[agent.py](https://github.com/xrsrke/instructGOOSE/blob/d6b35b5a571a3933cb4b2059a6b94839b6a0571e/instruct_goose/agent.py#L94)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dfc1d8-34cd-4f7d-b38f-d7863aa5a486",
   "metadata": {},
   "source": [
    "::: {.column-margin}\n",
    "Page 9. Formula 2\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1a29a6-b89c-4eeb-badf-0835cf2a1cd4",
   "metadata": {},
   "source": [
    "Question 2: What does $\\sigma\\left(r_\\theta\\left(x, y_w\\right)-r_\\theta\\left(x, y_l\\right)\\right)$ measure?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1901404a-5d35-441f-9ded-c1b62314f09a",
   "metadata": {},
   "source": [
    "According to the paper, $y_w$ is the output preferred by the human labeler, while $y_l$ is the output that is not preferred by the human labeler\n",
    "\n",
    "If the reward model predicts that\n",
    "\n",
    "- The score for $y_w$ **is larger than** the score for $y_l$\n",
    "    - Then $r_\\theta\\left(x, y_w\\right)-r_\\theta\\left(x, y_l\\right) > 0$\n",
    "    - Then the output of the sigmoid function will be closer to 1, indicating a higher probability that the labeler will prefer $y_w$ (which is our target)\n",
    "- The score for $y_w$ **is less** **than** the score for $y_l$\n",
    "    - Then $r_\\theta\\left(x, y_w\\right)-r_\\theta\\left(x, y_l\\right) < 0$\n",
    "    - Then the output of the sigmoid function will be closer to 0, indicating a lower probability that the labeler will prefer $y_l$ (which isnâ€™t our target)\n",
    "\n",
    "The goal of this part is to measure the probability that the model correctly predicts the target output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a987699-ff65-4ba9-a705-07862c601dd9",
   "metadata": {},
   "source": [
    "-----\n",
    "Question 3: Why do we minus $\\pi_\\phi^{\\mathrm{RL}}(y \\mid x) / \\pi^{\\mathrm{SFT}}(y \\mid x)$?\n",
    "\n",
    "- Because encourage the RL-based model to generate sequences that are similar to those generated by the SFT model.\n",
    "- A smaller output of this term in the equation corresponds to a larger objective, which aligns with the goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94cc2dd-c658-4229-94b5-329e37da7c5a",
   "metadata": {},
   "source": [
    "-----\n",
    "Question 4: What does $E_{x \\sim D_{\\text {pretrain }}}\\left[\\log \\left(\\pi_\\phi^{\\mathrm{RL}}(x)\\right)\\right]$ measure? Explain\n",
    "\n",
    "It measures the coherence in the generated text. Because\n",
    "\n",
    "- The sum of the log probabilities of each token in the generated text can be used as a measure of how well the text aligns with the patterns in the training data\n",
    "- If the sum of log probabilities is high, it indicates that the generated text is more likely to have been drawn from the training data, and therefore is more coherent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
