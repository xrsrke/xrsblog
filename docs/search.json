[
  {
    "objectID": "posts/03_rlhf/Untitled.html",
    "href": "posts/03_rlhf/Untitled.html",
    "title": "xrs",
    "section": "",
    "text": "import matplotlib.pyplot as plt\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\n\nBreaking down RLHF\noutput\n\nAlign with human preferences\nTweak the training loop of GPT\nSupervise Fine-tuning (SFT): the model is used to fined tune with prompts and desired output by labelers\nReward Model (RM)\n\n\nStart with\n\nA pretrained language model\nA distribution of prompts on which we want our model to produce aligned outputs\nA team of trained human labelers\n\n\n\nSteps\n\nCompare output of difference language model on the dataset\nLabeler scores each output from these model\nReward model learn to predicts which model output that labelers would prefer\nUse reward model as reward function\nFine-tune pre-trained model to maximize the reward using PPO\n\n\n\n\nDateset\n\n\nThe idea\nAlign with human preferences\n\nReward Model\n\nNotations\n\n\nReward Model\nOutline - Why take the negative and log - Why minus - Why take the sigmoid\n\\(\\operatorname{loss}(\\theta)=-\\frac{1}{\\left(\\begin{array}{c} K \\\\ 2 \\end{array}\\right)} E_{\\left(x, y_w, y_l\\right) \\sim D}\\left[\\log \\left(\\sigma\\left(r_\\theta\\left(x, y_w\\right)-r_\\theta\\left(x, y_l\\right)\\right)\\right)\\right]\\)\n\\(y_w\\), and \\(y_l\\): represent the output that prefered and non-prefereed by human labeller respectively\nThe goal of the reward model is make it algin with human preference as much as possible. So we want a loss score that indicate if the loss is high that mean the reward model isn’t doing good its job and vice versa. - \\(r_\\theta\\left(x, y_w\\right)\\) is the reward scalar of the reward model for prompt \\(x\\) and the summary \\(y_w\\) - \\(r_\\theta\\left(x, y_l\\right)\\)\n\n\nSigmoid Recap\n\nxs = torch.arange(-2, 2, step=0.1)\n\n\nplt.plot(xs.numpy(), torch.sigmoid(xs).numpy())\n\n\n\n\n\nInput: query and answer\nOutput: score (scalar)\n\nModels: GPT-J, GPT-Neo\nLoss: Cross-entropy. Which output labelers prefer\n\nDataset\npresent labelers with multiple responses to a prompt, and ask them to rank the responses\nhttps://huggingface.co/datasets/openai/webgpt_comparisons\n\n\nLoss function\n10 responses. compare pair\n\n\n\nSFT Model\n\nhttps://huggingface.co/Dahoas/gpt2-sft-static\nhttps://huggingface.co/Dahoas/gptneo-sft-static\nhttps://huggingface.co/Dahoas/gptj-sft-static\n\n\nimport torch\nimport torch.nn.functional as F\n\n\nx1 = torch.tensor(-0.1)\nx2 = torch.tensor(0.3)\nx3 = torch.tensor(0.5)\nx4 = torch.tensor(0.8)\n\n\ny = torch.tensor(0.3)\n\n\ny2 = torch.tensor(0.9)\n\n\nF.sigmoid(x)\n\ntensor(0.5498)\n\n\n\nF.sigmoid(y).log()\n\ntensor(-0.5544)\n\n\n\nF.sigmoid(y2).log()\n\ntensor(-0.3412)\n\n\n\nmaximize log likelihood = minimize negative log likelihood\n\n\nF.sigmoid(x1), F.sigmoid(x2), F.sigmoid(x3), F.sigmoid(x4)\n\n(tensor(0.4750), tensor(0.5744), tensor(0.6225), tensor(0.6900))\n\n\n\ntorch.tensor(1).log()\n\ntensor(0.)"
  },
  {
    "objectID": "posts/04_protein/latex.html",
    "href": "posts/04_protein/latex.html",
    "title": "xrs",
    "section": "",
    "text": "The loss function described in the text is used to train a Transformer-based neural network for protein generation. The goal of the model is to generate a sequence of amino acids, which together specify a protein, while being controlled by a set of descriptors known as control tags.\nThe variable \\(a\\) represents a sequence of amino acids and has length \\(n_a-1\\). The variable \\(c\\) represents the control tags, which are a set of descriptors such as protein family or source organism, and has length \\(n_c\\). The variable \\(x\\) is formed by prepending the control tag sequence to the amino acid sequence and has length \\(n=n_a+n_c\\). The probability over such a combined sequence is represented by \\(P(x)\\).\nThe language modeling problem is decomposed into a next-token prediction problem, where a token can either be an amino acid or a control tag. The neural network has parameters \\(\\theta\\) and is trained to minimize the negative log-likelihood over a dataset \\(D={x^1, \\ldots, x^{|D|}}\\). The loss function is calculated as follows:\n� ( � ) = − 1 ∣ � ∣ ∑ � = 1 ∣ � ∣ 1 � � ∑ � = 1 � � log ⁡ � � ( � � � ∣ � < � � ) L(D)=− ∣D∣ 1 ​\nk=1 ∑ ∣D∣ ​\nn k\n1 ​\ni=1 ∑ n k\n​ logp θ ​ (x i k ​ ∣x <i k ​ ) Once the neural network is trained, it can generate a new protein \\(\\underline{a}\\) of length \\(m_a\\) controlled by a control tag sequence \\(\\underline{c}\\) of length \\(m_c\\). The generation process starts by sequentially sampling its constituent tokens based on the learned parameters \\(\\theta\\). This is done by calculating the probabilities \\(p_\\theta\\left(a_1 \\mid \\underline{c}\\right), p_\\theta\\left(a_2 \\mid a_1, \\underline{c}\\right), \\ldots, p_\\theta\\left(a_j \\mid a_{<j}, \\underline{c}\\right)\\). The generation process continues until the model generates an ‘end of sequence’ token."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BLOG",
    "section": "",
    "text": "Current Projects\nI’m implementing RLHF (ChatGPT) and MuZero\nAbout me\nI am interested in particle physics, deep learning, neuroscience, material science, nanoscience, astrophysics and synthetic biology. I have designed my curriculum and study from 4:00 AM to 3:00 PM to get the information into my brain. Last month, I spent 200 hours studying consistently (24/7/365).\nI am streaming daily on Twitch: twitch.tv/xrsrke\nReach me? Just drop a message neuralink#7014 in discord, twitter/@xariusrke\n\n\n\n\n\n\n\n\n\n\n\nBreaking down RLHF\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n)\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "project_foundation.html",
    "href": "project_foundation.html",
    "title": "Project Foundation",
    "section": "",
    "text": "Currently I’m full-time working on Project Foundation.\nThe goal of project foundation is give me a foundation to give me a framework of thinking…\n\nPhysics\nDeep Learning\nMaterial Science\nNanoscience\nNeuroscience"
  }
]