[
  {
    "objectID": "posts/03_rlhf/Untitled.html",
    "href": "posts/03_rlhf/Untitled.html",
    "title": "xrs",
    "section": "",
    "text": "import matplotlib.pyplot as plt\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\n\nBreaking down RLHF\noutput\n\nAlign with human preferences\nTweak the training loop of GPT\nSupervise Fine-tuning (SFT): the model is used to fined tune with prompts and desired output by labelers\nReward Model (RM)\n\n\nStart with\n\nA pretrained language model\nA distribution of prompts on which we want our model to produce aligned outputs\nA team of trained human labelers\n\n\n\nSteps\n\nCompare output of difference language model on the dataset\nLabeler scores each output from these model\nReward model learn to predicts which model output that labelers would prefer\nUse reward model as reward function\nFine-tune pre-trained model to maximize the reward using PPO\n\n\n\n\nDateset\n\n\nThe idea\nAlign with human preferences\n\nReward Model\n\nNotations\n\n\nReward Model\nOutline - Why take the negative and log - Why minus - Why take the sigmoid\n\\(\\operatorname{loss}(\\theta)=-\\frac{1}{\\left(\\begin{array}{c} K \\\\ 2 \\end{array}\\right)} E_{\\left(x, y_w, y_l\\right) \\sim D}\\left[\\log \\left(\\sigma\\left(r_\\theta\\left(x, y_w\\right)-r_\\theta\\left(x, y_l\\right)\\right)\\right)\\right]\\)\n\\(y_w\\), and \\(y_l\\): represent the output that prefered and non-prefereed by human labeller respectively\nThe goal of the reward model is make it algin with human preference as much as possible. So we want a loss score that indicate if the loss is high that mean the reward model isn’t doing good its job and vice versa. - \\(r_\\theta\\left(x, y_w\\right)\\) is the reward scalar of the reward model for prompt \\(x\\) and the summary \\(y_w\\) - \\(r_\\theta\\left(x, y_l\\right)\\)\n\n\nSigmoid Recap\n\nxs = torch.arange(-2, 2, step=0.1)\n\n\nplt.plot(xs.numpy(), torch.sigmoid(xs).numpy())\n\n\n\n\n\nInput: query and answer\nOutput: score (scalar)\n\nModels: GPT-J, GPT-Neo\nLoss: Cross-entropy. Which output labelers prefer\n\nDataset\npresent labelers with multiple responses to a prompt, and ask them to rank the responses\nhttps://huggingface.co/datasets/openai/webgpt_comparisons\n\n\nLoss function\n10 responses. compare pair\n\n\n\nSFT Model\n\nhttps://huggingface.co/Dahoas/gpt2-sft-static\nhttps://huggingface.co/Dahoas/gptneo-sft-static\nhttps://huggingface.co/Dahoas/gptj-sft-static\n\n\nimport torch\nimport torch.nn.functional as F\n\n\nx1 = torch.tensor(-0.1)\nx2 = torch.tensor(0.3)\nx3 = torch.tensor(0.5)\nx4 = torch.tensor(0.8)\n\n\ny = torch.tensor(0.3)\n\n\ny2 = torch.tensor(0.9)\n\n\nF.sigmoid(x)\n\ntensor(0.5498)\n\n\n\nF.sigmoid(y).log()\n\ntensor(-0.5544)\n\n\n\nF.sigmoid(y2).log()\n\ntensor(-0.3412)\n\n\n\nmaximize log likelihood = minimize negative log likelihood\n\n\nF.sigmoid(x1), F.sigmoid(x2), F.sigmoid(x3), F.sigmoid(x4)\n\n(tensor(0.4750), tensor(0.5744), tensor(0.6225), tensor(0.6900))\n\n\n\ntorch.tensor(1).log()\n\ntensor(0.)"
  },
  {
    "objectID": "posts/04_protein/latex.html",
    "href": "posts/04_protein/latex.html",
    "title": "xrs",
    "section": "",
    "text": "The loss function described in the text is used to train a Transformer-based neural network for protein generation. The goal of the model is to generate a sequence of amino acids, which together specify a protein, while being controlled by a set of descriptors known as control tags.\nThe variable \\(a\\) represents a sequence of amino acids and has length \\(n_a-1\\). The variable \\(c\\) represents the control tags, which are a set of descriptors such as protein family or source organism, and has length \\(n_c\\). The variable \\(x\\) is formed by prepending the control tag sequence to the amino acid sequence and has length \\(n=n_a+n_c\\). The probability over such a combined sequence is represented by \\(P(x)\\).\nThe language modeling problem is decomposed into a next-token prediction problem, where a token can either be an amino acid or a control tag. The neural network has parameters \\(\\theta\\) and is trained to minimize the negative log-likelihood over a dataset \\(D={x^1, \\ldots, x^{|D|}}\\). The loss function is calculated as follows:\n� ( � ) = − 1 ∣ � ∣ ∑ � = 1 ∣ � ∣ 1 � � ∑ � = 1 � � log ⁡ � � ( � � � ∣ � < � � ) L(D)=− ∣D∣ 1 ​\nk=1 ∑ ∣D∣ ​\nn k\n1 ​\ni=1 ∑ n k\n​ logp θ ​ (x i k ​ ∣x <i k ​ ) Once the neural network is trained, it can generate a new protein \\(\\underline{a}\\) of length \\(m_a\\) controlled by a control tag sequence \\(\\underline{c}\\) of length \\(m_c\\). The generation process starts by sequentially sampling its constituent tokens based on the learned parameters \\(\\theta\\). This is done by calculating the probabilities \\(p_\\theta\\left(a_1 \\mid \\underline{c}\\right), p_\\theta\\left(a_2 \\mid a_1, \\underline{c}\\right), \\ldots, p_\\theta\\left(a_j \\mid a_{<j}, \\underline{c}\\right)\\). The generation process continues until the model generates an ‘end of sequence’ token."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BLOG",
    "section": "",
    "text": "Current Projects\nI’m implementing RLHF (ChatGPT) and MuZero\nAbout me\nI am interested in particle physics, deep learning, neuroscience, material science, nanoscience, astrophysics and synthetic biology. I have designed my curriculum and study from 4:00 AM to 3:00 PM to get the information into my brain. Last month, I spent 200 hours studying consistently (24/7/365).\nI am streaming daily on Twitch: twitch.tv/xrsrke\nReach me? Just drop a message neuralink#7014 in discord, twitter/@xariusrke\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n)\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nHow RLHF works?\n\n\n\n\n\nA summary of the best practices for summarizing output of reproducible scientific documents.\n\n\n\n\n\n\nMar 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest\n\n\n\n\n\nA summary of the best practices for summarizing output of reproducible scientific documents.\n\n\n\n\n\n\nMar 30, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "project_foundation.html",
    "href": "project_foundation.html",
    "title": "Project Foundation",
    "section": "",
    "text": "Currently I’m full-time working on Project Foundation.\nThe goal of project foundation is give me a foundation to give me a framework of thinking…\n\nPhysics\nDeep Learning\nMaterial Science\nNanoscience\nNeuroscience"
  },
  {
    "objectID": "posts/03_rlhf/post.html",
    "href": "posts/03_rlhf/post.html",
    "title": "xrs",
    "section": "",
    "text": "import matplotlib.pyplot as plt\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\n\nBreaking down RLHF\noutput\n\nAlign with human preferences\nTweak the training loop of GPT\nSupervise Fine-tuning (SFT): the model is used to fined tune with prompts and desired output by labelers\nReward Model (RM)\n\n\nStart with\n\nA pretrained language model\nA distribution of prompts on which we want our model to produce aligned outputs\nA team of trained human labelers\n\n\n\nSteps\n\nCompare output of difference language model on the dataset\nLabeler scores each output from these model\nReward model learn to predicts which model output that labelers would prefer\nUse reward model as reward function\nFine-tune pre-trained model to maximize the reward using PPO\n\n\n\n\nDateset\n\n\nThe idea\nAlign with human preferences\n\nReward Model\n\nNotations\n\n\nReward Model\nOutline - Why take the negative and log - Why minus - Why take the sigmoid\n\\(\\operatorname{loss}(\\theta)=-\\frac{1}{\\left(\\begin{array}{c} K \\\\ 2 \\end{array}\\right)} E_{\\left(x, y_w, y_l\\right) \\sim D}\\left[\\log \\left(\\sigma\\left(r_\\theta\\left(x, y_w\\right)-r_\\theta\\left(x, y_l\\right)\\right)\\right)\\right]\\)\n\\(y_w\\), and \\(y_l\\): represent the output that prefered and non-prefereed by human labeller respectively\nThe goal of the reward model is make it algin with human preference as much as possible. So we want a loss score that indicate if the loss is high that mean the reward model isn’t doing good its job and vice versa. - \\(r_\\theta\\left(x, y_w\\right)\\) is the reward scalar of the reward model for prompt \\(x\\) and the summary \\(y_w\\) - \\(r_\\theta\\left(x, y_l\\right)\\)\n\n\nSigmoid Recap\n\nxs = torch.arange(-2, 2, step=0.1)\n\n\nplt.plot(xs.numpy(), torch.sigmoid(xs).numpy())\n\n\n\n\n\nInput: query and answer\nOutput: score (scalar)\n\nModels: GPT-J, GPT-Neo\nLoss: Cross-entropy. Which output labelers prefer\n\nDataset\npresent labelers with multiple responses to a prompt, and ask them to rank the responses\nhttps://huggingface.co/datasets/openai/webgpt_comparisons\n\n\nLoss function\n10 responses. compare pair\n\n\n\nSFT Model\n\nhttps://huggingface.co/Dahoas/gpt2-sft-static\nhttps://huggingface.co/Dahoas/gptneo-sft-static\nhttps://huggingface.co/Dahoas/gptj-sft-static\n\n\nimport torch\nimport torch.nn.functional as F\n\n\nx1 = torch.tensor(-0.1)\nx2 = torch.tensor(0.3)\nx3 = torch.tensor(0.5)\nx4 = torch.tensor(0.8)\n\n\ny = torch.tensor(0.3)\n\n\ny2 = torch.tensor(0.9)\n\n\nF.sigmoid(x)\n\ntensor(0.5498)\n\n\n\nF.sigmoid(y).log()\n\ntensor(-0.5544)\n\n\n\nF.sigmoid(y2).log()\n\ntensor(-0.3412)\n\n\n\nmaximize log likelihood = minimize negative log likelihood\n\n\nF.sigmoid(x1), F.sigmoid(x2), F.sigmoid(x3), F.sigmoid(x4)\n\n(tensor(0.4750), tensor(0.5744), tensor(0.6225), tensor(0.6900))\n\n\n\ntorch.tensor(1).log()\n\ntensor(0.)"
  },
  {
    "objectID": "posts/post.html",
    "href": "posts/post.html",
    "title": "xrs",
    "section": "",
    "text": "import matplotlib.pyplot as plt\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\n\nBreaking down RLHF\noutput\n\nAlign with human preferences\nTweak the training loop of GPT\nSupervise Fine-tuning (SFT): the model is used to fined tune with prompts and desired output by labelers\nReward Model (RM)\n\n\nStart with\n\nA pretrained language model\nA distribution of prompts on which we want our model to produce aligned outputs\nA team of trained human labelers\n\n\n\nSteps\n\nCompare output of difference language model on the dataset\nLabeler scores each output from these model\nReward model learn to predicts which model output that labelers would prefer\nUse reward model as reward function\nFine-tune pre-trained model to maximize the reward using PPO\n\n\n\n\nDateset\n\n\nThe idea\nAlign with human preferences\n\nReward Model\n\nNotations\n\n\nReward Model\nOutline - Why take the negative and log - Why minus - Why take the sigmoid\n\\(\\operatorname{loss}(\\theta)=-\\frac{1}{\\left(\\begin{array}{c} K \\\\ 2 \\end{array}\\right)} E_{\\left(x, y_w, y_l\\right) \\sim D}\\left[\\log \\left(\\sigma\\left(r_\\theta\\left(x, y_w\\right)-r_\\theta\\left(x, y_l\\right)\\right)\\right)\\right]\\)\n\\(y_w\\), and \\(y_l\\): represent the output that prefered and non-prefereed by human labeller respectively\nThe goal of the reward model is make it algin with human preference as much as possible. So we want a loss score that indicate if the loss is high that mean the reward model isn’t doing good its job and vice versa. - \\(r_\\theta\\left(x, y_w\\right)\\) is the reward scalar of the reward model for prompt \\(x\\) and the summary \\(y_w\\) - \\(r_\\theta\\left(x, y_l\\right)\\)\n\n\nSigmoid Recap\n\nxs = torch.arange(-2, 2, step=0.1)\n\n\nplt.plot(xs.numpy(), torch.sigmoid(xs).numpy())\n\n\n\n\n\nInput: query and answer\nOutput: score (scalar)\n\nModels: GPT-J, GPT-Neo\nLoss: Cross-entropy. Which output labelers prefer\n\nDataset\npresent labelers with multiple responses to a prompt, and ask them to rank the responses\nhttps://huggingface.co/datasets/openai/webgpt_comparisons\n\n\nLoss function\n10 responses. compare pair\n\n\n\nSFT Model\n\nhttps://huggingface.co/Dahoas/gpt2-sft-static\nhttps://huggingface.co/Dahoas/gptneo-sft-static\nhttps://huggingface.co/Dahoas/gptj-sft-static\n\n\nimport torch\nimport torch.nn.functional as F\n\n\nx1 = torch.tensor(-0.1)\nx2 = torch.tensor(0.3)\nx3 = torch.tensor(0.5)\nx4 = torch.tensor(0.8)\n\n\ny = torch.tensor(0.3)\n\n\ny2 = torch.tensor(0.9)\n\n\nF.sigmoid(x)\n\ntensor(0.5498)\n\n\n\nF.sigmoid(y).log()\n\ntensor(-0.5544)\n\n\n\nF.sigmoid(y2).log()\n\ntensor(-0.3412)\n\n\n\nmaximize log likelihood = minimize negative log likelihood\n\n\nF.sigmoid(x1), F.sigmoid(x2), F.sigmoid(x3), F.sigmoid(x4)\n\n(tensor(0.4750), tensor(0.5744), tensor(0.6225), tensor(0.6900))\n\n\n\ntorch.tensor(1).log()\n\ntensor(0.)"
  },
  {
    "objectID": "posts/rlhf.html",
    "href": "posts/rlhf.html",
    "title": "How RLHF works?",
    "section": "",
    "text": "::: {.cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n:::\n\nHow RLHF works?\nPaper: Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback\n\n\n\nopenai-rlhf.png\n\n\nOutline - Why ChatGPT take does world by storm? + Train on million of text + Not align with the human intention + LM was trained on milition of articles from the internet.\n\n\nExplaining RLHF for a kid\n\n\nBreaking down RLHF\n\nNotations\n\n\n\nWhat does it represent\n\n\n\n\n12\n12\n\n\n123\n123\n\n\n1\n1\n\n\n\n#| label: tbl-planets\n#| tbl-cap: Planets\n\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\ntable = [[\"Sun\",696000,1989100000],\n         [\"Earth\",6371,5973.6],\n         [\"Moon\",1737,73.5],\n         [\"Mars\",3390,641.85]]\nMarkdown(tabulate(\n  table, \n  headers=[\"Planet\",\"R (km)\", \"mass (x 10^29 kg)\"]\n))\noutput\n\nAlign with human preferences\nTweak the training loop of GPT\nSupervise Fine-tuning (SFT): the model is used to fined tune with prompts and desired output by labelers\nReward Model (RM)\n\n\nStart with\n\nA pretrained language model\nA distribution of prompts on which we want our model to produce aligned outputs\nA team of trained human labelers\n\n\n\nSteps\n\nCompare output of difference language model on the dataset\nLabeler scores each output from these model\nReward model learn to predicts which model output that labelers would prefer\nUse reward model as reward function\nFine-tune pre-trained model to maximize the reward using PPO\n\n\n\n\nDateset\n\n\nThe idea\nAlign with human preferences\n\nReward Model\n\nNotations\n\n\nTraining the reward model\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"CarperAI/openai_summarize_comparisons\", split=\"train\")\n\nUsing custom data configuration CarperAI--openai_summarize_comparisons-79d2c222a15dc8fb\nFound cached dataset parquet (/Users/education/.cache/huggingface/datasets/CarperAI___parquet/CarperAI--openai_summarize_comparisons-79d2c222a15dc8fb/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n\n\n\\(\\operatorname{loss}(\\theta)=-\\frac{1}{\\left(\\begin{array}{c} K \\\\ 2 \\end{array}\\right)} E_{\\left(x, y_w, y_l\\right) \\sim D}\\left[\\log \\left(\\sigma\\left(r_\\theta\\left(x, y_w\\right)-r_\\theta\\left(x, y_l\\right)\\right)\\right)\\right]\\)\n\n\nPage 8. Formula 1\nclass PairwiseLoss(nn.Module):\n    def forward(\n        self,\n        chosen_rewards: TensorType[\"batch_size\", 1],\n        rejected_rewards: TensorType[\"batch_size\", 1]\n    ) -> TensorType[1]: # A scalar loss\n        assert len(chosen_rewards) == len(rejected_rewards)\n        batch_size = len(chosen_rewards)\n        \n        # maps the difference between the rewards to a probability\n        probs = torch.sigmoid(chosen_rewards - rejected_rewards)\n        return -probs.mean() / batch_size\n\n\nreward.py\n\\(y_w\\), and \\(y_l\\): represent the output that prefered and non-prefereed by human labeller respectively\nThe goal of the reward model is make it algin with human preference as much as possible. So we want a loss score that indicate if the loss is high that mean the reward model isn’t doing good its job and vice versa. - \\(r_\\theta\\left(x, y_w\\right)\\) is the reward scalar of the reward model for prompt \\(x\\) and the summary \\(y_w\\) - \\(r_\\theta\\left(x, y_l\\right)\\)\n\nDataset\npresent labelers with multiple responses to a prompt, and ask them to rank the responses\nhttps://huggingface.co/datasets/openai/webgpt_comparisons\n\n\nLoss function\n10 responses. compare pair\n\n\n\nSFT Model\n\nhttps://huggingface.co/Dahoas/gpt2-sft-static\nhttps://huggingface.co/Dahoas/gptneo-sft-static\nhttps://huggingface.co/Dahoas/gptj-sft-static\n\n\nimport torch\nimport torch.nn.functional as F\n\n\nx1 = torch.tensor(-0.1)\nx2 = torch.tensor(0.3)\nx3 = torch.tensor(0.5)\nx4 = torch.tensor(0.8)\n\n\ny = torch.tensor(0.3)\n\n\ny2 = torch.tensor(0.9)\n\n\nF.sigmoid(x)\n\ntensor(0.5498)\n\n\n\nF.sigmoid(y).log()\n\ntensor(-0.5544)\n\n\n\nF.sigmoid(y2).log()\n\ntensor(-0.3412)\n\n\n\nmaximize log likelihood = minimize negative log likelihood\n\n\nF.sigmoid(x1), F.sigmoid(x2), F.sigmoid(x3), F.sigmoid(x4)\n\n(tensor(0.4750), tensor(0.5744), tensor(0.6225), tensor(0.6900))\n\n\n\ntorch.tensor(1).log()\n\ntensor(0.)\n\n\n\n\nObjective Function\n\\(\\begin{aligned}\\text { objective }(\\phi)= & E_{(x, y) \\sim D_{\\pi_\\phi^{\\mathrm{RL}}}}\\left[r_\\theta(x, y)-\\beta \\log \\left(\\pi_\\phi^{\\mathrm{RL}}(y \\mid x) / \\pi^{\\mathrm{SFT}}(y \\mid x)\\right)\\right]+ \\\\& \\gamma E_{x \\sim D_{\\text {pretrain }}}\\left[\\log \\left(\\pi_\\phi^{\\mathrm{RL}}(x)\\right)\\right]\\end{aligned}\\)\nclass AgentObjective(nn.Module):\n    \"\"\"Agent objective.\"\"\"\n    def __init__(\n        self,\n        model: Callable, # the language model\n        sft_model: Callable, # the reference model\n        reward_model: Callable, # the reward model\n        gamma: float,\n        beta: float\n    ):\n        super().__init__()\n        self.model = model\n        self.sft_model = sft_model\n        self.reward_model = reward_model\n        self.gamma = gamma\n        self.beta = beta\n        \n    def forward(\n        self,\n        input_ids: TensorType[\"batch_size\", \"seq_len\"],\n        attention_mask: TensorType[\"batch_size\", \"seq_len\"]\n    ) -> TensorType[1]: # A scalar objective value\n        \"\"\"Calculate the objective value given the input ids and attention mask.\"\"\"\n        model_logits = self.model(input_ids, attention_mask)\n        model_dist = F.softmax(model_logits, dim=-1)\n        \n        sft_logits = self.sft_model(input_ids, attention_mask)\n        sft_dist = F.softmax(sft_logits, dim=-1)\n        \n        reward_score = self.reward_model(input_ids, attention_mask)\n        ratio = torch.log(model_dist / sft_dist)\n        \n        # compute the coherent of the generated text\n        coherent = torch.log(model_dist)\n        objective = (reward_score - self.beta*ratio).mean() + self.gamma * coherent.mean()\n        return objective\n\n\nagent.py\n\nPage 9. Formula 2\n\nQuestion 2: What does \\(\\sigma\\left(r_\\theta\\left(x, y_w\\right)-r_\\theta\\left(x, y_l\\right)\\right)\\) measure?\nAccording to the paper, \\(y_w\\) is the output preferred by the human labeler, while \\(y_l\\) is the output that is not preferred by the human labeler\nIf the reward model predicts that\n\nThe score for \\(y_w\\) is larger than the score for \\(y_l\\)\n\nThen \\(r_\\theta\\left(x, y_w\\right)-r_\\theta\\left(x, y_l\\right) > 0\\)\nThen the output of the sigmoid function will be closer to 1, indicating a higher probability that the labeler will prefer \\(y_w\\) (which is our target)\n\nThe score for \\(y_w\\) is less than the score for \\(y_l\\)\n\nThen \\(r_\\theta\\left(x, y_w\\right)-r_\\theta\\left(x, y_l\\right) < 0\\)\nThen the output of the sigmoid function will be closer to 0, indicating a lower probability that the labeler will prefer \\(y_l\\) (which isn’t our target)\n\n\nThe goal of this part is to measure the probability that the model correctly predicts the target output\n\n\n\n\n\n\nQuestion 3: Why do we minus \\(\\pi_\\phi^{\\mathrm{RL}}(y \\mid x) / \\pi^{\\mathrm{SFT}}(y \\mid x)\\)?\n\n\n- Because encourage the RL-based model to generate sequences that are similar to those generated by the SFT model. - A smaller output of this term in the equation corresponds to a larger objective, which aligns with the goal\n\n\n\nQuestion 4: What does \\(E_{x \\sim D_{\\text {pretrain }}}\\left[\\log \\left(\\pi_\\phi^{\\mathrm{RL}}(x)\\right)\\right]\\) measure? Explain\nIt measures the coherence in the generated text. Because\n\nThe sum of the log probabilities of each token in the generated text can be used as a measure of how well the text aligns with the patterns in the training data\nIf the sum of log probabilities is high, it indicates that the generated text is more likely to have been drawn from the training data, and therefore is more coherent."
  },
  {
    "objectID": "posts/rlhf.html#objective-function",
    "href": "posts/rlhf.html#objective-function",
    "title": "How RLHF works?",
    "section": "Objective Function",
    "text": "Objective Function\n\nObjective Function\n\\(\\begin{aligned}\\text { objective }(\\phi)= & E_{(x, y) \\sim D_{\\pi_\\phi^{\\mathrm{RL}}}}\\left[r_\\theta(x, y)-\\beta \\log \\left(\\pi_\\phi^{\\mathrm{RL}}(y \\mid x) / \\pi^{\\mathrm{SFT}}(y \\mid x)\\right)\\right]+ \\\\& \\gamma E_{x \\sim D_{\\text {pretrain }}}\\left[\\log \\left(\\pi_\\phi^{\\mathrm{RL}}(x)\\right)\\right]\\end{aligned}\\)\n\n\nPage 9. Formula 2\nQuestion 2: What does \\(\\sigma\\left(r_\\theta\\left(x, y_w\\right)-r_\\theta\\left(x, y_l\\right)\\right)\\) measure?\nAccording to the paper, \\(y_w\\) is the output preferred by the human labeler, while \\(y_l\\) is the output that is not preferred by the human labeler\nIf the reward model predicts that\n\nThe score for \\(y_w\\) is larger than the score for \\(y_l\\)\n\nThen \\(r_\\theta\\left(x, y_w\\right)-r_\\theta\\left(x, y_l\\right) > 0\\)\nThen the output of the sigmoid function will be closer to 1, indicating a higher probability that the labeler will prefer \\(y_w\\) (which is our target)\n\nThe score for \\(y_w\\) is less than the score for \\(y_l\\)\n\nThen \\(r_\\theta\\left(x, y_w\\right)-r_\\theta\\left(x, y_l\\right) < 0\\)\nThen the output of the sigmoid function will be closer to 0, indicating a lower probability that the labeler will prefer \\(y_l\\) (which isn’t our target)\n\n\nThe goal of this part is to measure the probability that the model correctly predicts the target output\n\nQuestion 3: Why take the negative log of sigmoid’s output?\nSo our goal is to maximize the \\(\\sigma(...)\\). That’s equivalent to minimize the negative of the log of \\(\\sigma\\left(...\\right)\\)\nPaper: Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback\n\nimport matplotlib.pyplot as plt\nplt.plot([1,2,3,4])\nplt.show()"
  },
  {
    "objectID": "posts/04_protein/Untitled.html",
    "href": "posts/04_protein/Untitled.html",
    "title": "xrs",
    "section": "",
    "text": "sp|Q6GZX4|001R_FRG3G Putative transcription factor 001R OS=Frog virus 3 (isolate Goorha) OX=654924 GN=FV3-001R PE=4 SV=1"
  },
  {
    "objectID": "posts/05_the_brain.html",
    "href": "posts/05_the_brain.html",
    "title": "xrs",
    "section": "",
    "text": "Distribution\nThe brain is a distributed network Source\n\n\nNavigation\n\nPlace Cell\n\n\n\nVision\n\n\nContinual Learning\n\n\nBrain Development\n\n\nNeural Representation\n\nConcept Cells\n\nContext-independent\nBiological neuron are conceptual neurons. They firing for a specific concept regarless of the multimodal. [Source]\n\nEg: react to both written name, spoken names of their target subjects and not to other names\nWhen a neuron fired to Jennifer Aniston, they showed a picture of her in the Eiffel Tower and found that the neuron started firing to the Eiffel Tower as well, without Aniston needing to be in the picture\nThoughts\n\nBecause that’s the way we store our memories. We tend to remember concepts and associations between them and to forget irrelevant details — so, we’ll recall seeing pictures of Jennifer Aniston at the hospital ward, for instance, but won’t remember details of the specific images.\n\n\nSome neurons are specialize for recognize faces [Source]\nSome neurons are specialize firing for a particular concept. Eg: Jennifer Aniston\nConcept Cells respond to related items, helping to code meaningful associations for the patient, and support long-term episodic memory. [Source]\n\n\n\n\nMemory"
  }
]