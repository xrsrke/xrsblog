[
  {
    "objectID": "posts/03_the_brain.html",
    "href": "posts/03_the_brain.html",
    "title": "ðŸ§  The Brain",
    "section": "",
    "text": "Lastest update: April 01, 2023\n\nFace-selective neurons\n\nMultimodal Neurons\n\nBecause thatâ€™s the way we store our memories. We tend to remember concepts and associations between them and to forget irrelevant details - so, weâ€™ll recall seeing pictures of Jennifer Aniston at the hospital ward, for instance, but wonâ€™t remember details of the specific images.\n\nBiological neurons are multimodal neurons, firing in response to a specific concept regardless of its modality (Quiroga et al., 2005).\nSome neurons even fire for an item related to a specific person (De Falco et al., 2016), or image of the person regarless with their head orinatation (Freiwald et al, 2010).\n\n\nContinual Learning\nFace-selective neurons maintain the same distinctive selectivity patterns for at least 10 years (McMahon et al., 2014).\nCritical Point\n(Stringer et al., 2019) found that the neural representation in the mouseâ€™s visual cortex follows critical points."
  },
  {
    "objectID": "posts/rlhf.html",
    "href": "posts/rlhf.html",
    "title": "How RLHF works?",
    "section": "",
    "text": "::: {.cell 0=â€˜hâ€™ 1=â€˜iâ€™ 2=â€˜dâ€™ 3=â€˜eâ€™}\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n:::\n\nHow RLHF works?\nPaper: Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback\n\n\n\nopenai-rlhf.png\n\n\nOutline - Why ChatGPT take does world by storm? + Train on million of text + Not align with the human intention + LM was trained on milition of articles from the internet.\n\n\nExplaining RLHF for a kid\n\n\nBreaking down RLHF\n\nNotations\n\n\n\nWhat does it represent\n\n\n\n\n12\n12\n\n\n123\n123\n\n\n1\n1\n\n\n\n#| label: tbl-planets\n#| tbl-cap: Planets\n\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\ntable = [[\"Sun\",696000,1989100000],\n         [\"Earth\",6371,5973.6],\n         [\"Moon\",1737,73.5],\n         [\"Mars\",3390,641.85]]\nMarkdown(tabulate(\n  table, \n  headers=[\"Planet\",\"R (km)\", \"mass (x 10^29 kg)\"]\n))\noutput\n\nAlign with human preferences\nTweak the training loop of GPT\nSupervise Fine-tuning (SFT): the model is used to fined tune with prompts and desired output by labelers\nReward Model (RM)\n\n\nStart with\n\nA pretrained language model\nA distribution of prompts on which we want our model to produce aligned outputs\nA team of trained human labelers\n\n\n\nSteps\n\nCompare output of difference language model on the dataset\nLabeler scores each output from these model\nReward model learn to predicts which model output that labelers would prefer\nUse reward model as reward function\nFine-tune pre-trained model to maximize the reward using PPO\n\n\n\n\nDateset\n\n\nThe idea\nAlign with human preferences\n\nReward Model\n\nNotations\n\n\nTraining the reward model\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"CarperAI/openai_summarize_comparisons\", split=\"train\")\n\nUsing custom data configuration CarperAI--openai_summarize_comparisons-79d2c222a15dc8fb\nFound cached dataset parquet (/Users/education/.cache/huggingface/datasets/CarperAI___parquet/CarperAI--openai_summarize_comparisons-79d2c222a15dc8fb/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n\n\n\\(\\operatorname{loss}(\\theta)=-\\frac{1}{\\left(\\begin{array}{c} K \\\\ 2 \\end{array}\\right)} E_{\\left(x, y_w, y_l\\right) \\sim D}\\left[\\log \\left(\\sigma\\left(r_\\theta\\left(x, y_w\\right)-r_\\theta\\left(x, y_l\\right)\\right)\\right)\\right]\\)\n\n\nPage 8. Formula 1\nclass PairwiseLoss(nn.Module):\n    def forward(\n        self,\n        chosen_rewards: TensorType[\"batch_size\", 1],\n        rejected_rewards: TensorType[\"batch_size\", 1]\n    ) -> TensorType[1]: # A scalar loss\n        assert len(chosen_rewards) == len(rejected_rewards)\n        batch_size = len(chosen_rewards)\n        \n        # maps the difference between the rewards to a probability\n        probs = torch.sigmoid(chosen_rewards - rejected_rewards)\n        return -probs.mean() / batch_size\n\n\nreward.py\n\\(y_w\\), and \\(y_l\\): represent the output that prefered and non-prefereed by human labeller respectively\nThe goal of the reward model is make it algin with human preference as much as possible. So we want a loss score that indicate if the loss is high that mean the reward model isnâ€™t doing good its job and vice versa. - \\(r_\\theta\\left(x, y_w\\right)\\) is the reward scalar of the reward model for prompt \\(x\\) and the summary \\(y_w\\) - \\(r_\\theta\\left(x, y_l\\right)\\)\n\nDataset\npresent labelers with multiple responses to a prompt, and ask them to rank the responses\nhttps://huggingface.co/datasets/openai/webgpt_comparisons\n\n\nLoss function\n10 responses. compare pair\n\n\n\nSFT Model\n\nhttps://huggingface.co/Dahoas/gpt2-sft-static\nhttps://huggingface.co/Dahoas/gptneo-sft-static\nhttps://huggingface.co/Dahoas/gptj-sft-static\n\n\nimport torch\nimport torch.nn.functional as F\n\n\nx1 = torch.tensor(-0.1)\nx2 = torch.tensor(0.3)\nx3 = torch.tensor(0.5)\nx4 = torch.tensor(0.8)\n\n\ny = torch.tensor(0.3)\n\n\ny2 = torch.tensor(0.9)\n\n\nF.sigmoid(x)\n\ntensor(0.5498)\n\n\n\nF.sigmoid(y).log()\n\ntensor(-0.5544)\n\n\n\nF.sigmoid(y2).log()\n\ntensor(-0.3412)\n\n\n\nmaximize log likelihood = minimize negative log likelihood\n\n\nF.sigmoid(x1), F.sigmoid(x2), F.sigmoid(x3), F.sigmoid(x4)\n\n(tensor(0.4750), tensor(0.5744), tensor(0.6225), tensor(0.6900))\n\n\n\ntorch.tensor(1).log()\n\ntensor(0.)\n\n\n\n\nObjective Function\n\\(\\begin{aligned}\\text { objective }(\\phi)= & E_{(x, y) \\sim D_{\\pi_\\phi^{\\mathrm{RL}}}}\\left[r_\\theta(x, y)-\\beta \\log \\left(\\pi_\\phi^{\\mathrm{RL}}(y \\mid x) / \\pi^{\\mathrm{SFT}}(y \\mid x)\\right)\\right]+ \\\\& \\gamma E_{x \\sim D_{\\text {pretrain }}}\\left[\\log \\left(\\pi_\\phi^{\\mathrm{RL}}(x)\\right)\\right]\\end{aligned}\\)\nclass AgentObjective(nn.Module):\n    \"\"\"Agent objective.\"\"\"\n    def __init__(\n        self,\n        model: Callable, # the language model\n        sft_model: Callable, # the reference model\n        reward_model: Callable, # the reward model\n        gamma: float,\n        beta: float\n    ):\n        super().__init__()\n        self.model = model\n        self.sft_model = sft_model\n        self.reward_model = reward_model\n        self.gamma = gamma\n        self.beta = beta\n        \n    def forward(\n        self,\n        input_ids: TensorType[\"batch_size\", \"seq_len\"],\n        attention_mask: TensorType[\"batch_size\", \"seq_len\"]\n    ) -> TensorType[1]: # A scalar objective value\n        \"\"\"Calculate the objective value given the input ids and attention mask.\"\"\"\n        model_logits = self.model(input_ids, attention_mask)\n        model_dist = F.softmax(model_logits, dim=-1)\n        \n        sft_logits = self.sft_model(input_ids, attention_mask)\n        sft_dist = F.softmax(sft_logits, dim=-1)\n        \n        reward_score = self.reward_model(input_ids, attention_mask)\n        ratio = torch.log(model_dist / sft_dist)\n        \n        # compute the coherent of the generated text\n        coherent = torch.log(model_dist)\n        objective = (reward_score - self.beta*ratio).mean() + self.gamma * coherent.mean()\n        return objective\n\n\nagent.py\n\nPage 9. Formula 2\n\nQuestion 2: What does \\(\\sigma\\left(r_\\theta\\left(x, y_w\\right)-r_\\theta\\left(x, y_l\\right)\\right)\\) measure?\nAccording to the paper, \\(y_w\\) is the output preferred by the human labeler, while \\(y_l\\) is the output that is not preferred by the human labeler\nIf the reward model predicts that\n\nThe score for \\(y_w\\) is larger than the score for \\(y_l\\)\n\nThen \\(r_\\theta\\left(x, y_w\\right)-r_\\theta\\left(x, y_l\\right) > 0\\)\nThen the output of the sigmoid function will be closer to 1, indicating a higher probability that the labeler will prefer \\(y_w\\) (which is our target)\n\nThe score for \\(y_w\\) is less than the score for \\(y_l\\)\n\nThen \\(r_\\theta\\left(x, y_w\\right)-r_\\theta\\left(x, y_l\\right) < 0\\)\nThen the output of the sigmoid function will be closer to 0, indicating a lower probability that the labeler will prefer \\(y_l\\) (which isnâ€™t our target)\n\n\nThe goal of this part is to measure the probability that the model correctly predicts the target output\n\n\n\n\n\n\nQuestion 3: Why do we minus \\(\\pi_\\phi^{\\mathrm{RL}}(y \\mid x) / \\pi^{\\mathrm{SFT}}(y \\mid x)\\)?\n\n\n- Because encourage the RL-based model to generate sequences that are similar to those generated by the SFT model. - A smaller output of this term in the equation corresponds to a larger objective, which aligns with the goal\n\n\n\nQuestion 4: What does \\(E_{x \\sim D_{\\text {pretrain }}}\\left[\\log \\left(\\pi_\\phi^{\\mathrm{RL}}(x)\\right)\\right]\\) measure? Explain\nIt measures the coherence in the generated text. Because\n\nThe sum of the log probabilities of each token in the generated text can be used as a measure of how well the text aligns with the patterns in the training data\nIf the sum of log probabilities is high, it indicates that the generated text is more likely to have been drawn from the training data, and therefore is more coherent."
  },
  {
    "objectID": "posts/02_ai_alignmet.html",
    "href": "posts/02_ai_alignmet.html",
    "title": "AI Alignment",
    "section": "",
    "text": "Human in the loop\nReinforcement Learning from Human Feedback\n\n\n\nRLHF\n\n\nThe loss function for the reward model\n\\(\\operatorname{loss}(\\theta)=-\\frac{1}{\\left(\\begin{array}{c}K \\\\ 2\\end{array}\\right)} E_{\\left(x, y_w, y_l\\right) \\sim D}\\left[\\log \\left(\\sigma\\left(r_\\theta\\left(x, y_w\\right)-r_\\theta\\left(x, y_l\\right)\\right)\\right)\\right]\\)\n\\(y_w\\) is the human-prefer label, and \\(y_l\\) is the non-human-prefer label. The goal is make the reward model \\(r_\\theta\\) assigns a higher score for \\(y_w\\) than \\(y_l\\). So we want to turn this score into a probability distribution\nThe objective function for the RL-based model\n\\(\\begin{aligned} \\operatorname{objective}(\\phi)= & E_{(x, y) \\sim D_{\\pi_\\phi^{\\mathrm{RL}}}}\\left[r_\\theta(x, y)-\\beta \\log \\left(\\pi_\\phi^{\\mathrm{RL}}(y \\mid x) / \\pi^{\\mathrm{SFT}}(y \\mid x)\\right)\\right]+ \\\\ & \\gamma E_{x \\sim D_{\\text {pretrain }}}\\left[\\log \\left(\\pi_\\phi^{\\mathrm{RL}}(x)\\right)\\right]\\end{aligned}\\)\n\n\nRed team\n\n\nSandwitching Experiment"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "XRS's Logs",
    "section": "",
    "text": "Just a place where I share my learning notes and progress! Most of my posts are ongoing, so they will be gradually updated.\nI stream daily on twitch: twitch.tv/xrsrke\nSay hello? Feel free to get in touch at neuralink#7014 in discord, twitter/@xariusrke or hello@xrs.wtf ðŸ˜Š \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nðŸ§  The Brain\n\n\n\n\n\nThere are problems in deep learning that we donâ€™t know how to solve (catastrophic forgeting, adversarial attackâ€¦). It makes sense to pay some attention to how the brain solves them\n\n\n\n\n\n\nApr 1, 2023\n\n\n\n\n\n\n  \n\n\n\n\nHow RLHF works?\n\n\n\n\n\nA summary of the best practices for summarizing output of reproducible scientific documents.\n\n\n\n\n\n\nMar 30, 2023\n\n\n\n\n\n\n  \n\n\n\n\nAI Alignment\n\n\n\n\n\nAI alignment is about making sure that AI systems work in line with what humans want, so that we can all benefit from it. As AI becomes more sophisticated, it is essential to ensure that it is working towards a shared and beneficial future for everyone.\n\n\n\n\n\n\nMar 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLanguage Models\n\n\n\n\n\n\n\n\n\n\n\n\nMar 30, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "project_foundation.html",
    "href": "project_foundation.html",
    "title": "Project Foundation",
    "section": "",
    "text": "Currently Iâ€™m full-time working on Project Foundation.\nThe goal of project foundation is give me a foundation to give me a framework of thinkingâ€¦\n\nPhysics\nDeep Learning\nMaterial Science\nNanoscience\nNeuroscience"
  }
]