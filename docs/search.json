[
  {
    "objectID": "posts/01_til/01_today_i_learned.html",
    "href": "posts/01_til/01_today_i_learned.html",
    "title": "Daily updates from Project Foundation",
    "section": "",
    "text": "Phase 7.5 (Miss deadline)\n11/15/002022 - reimplement positional encoding in transform from scratch\n11/22/002022 - How conv2d works and partial implement cnn from scratch\n11/24/00222 - Reimplement residual block from scratch\n11/26/002022 - Implement downsample and bottleneck residual block in ResNet from scratch\n11/27/002022 - Implement ResNet-34 from scratch"
  },
  {
    "objectID": "posts/03_rlhf/Untitled.html",
    "href": "posts/03_rlhf/Untitled.html",
    "title": "xrs",
    "section": "",
    "text": "import matplotlib.pyplot as plt\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\n\nBreaking down RLHF\noutput\n\nAlign with human preferences\nTweak the training loop of GPT\nSupervise Fine-tuning (SFT): the model is used to fined tune with prompts and desired output by labelers\nReward Model (RM)\n\n\nStart with\n\nA pretrained language model\nA distribution of prompts on which we want our model to produce aligned outputs\nA team of trained human labelers\n\n\n\nSteps\n\nCompare output of difference language model on the dataset\nLabeler scores each output from these model\nReward model learn to predicts which model output that labelers would prefer\nUse reward model as reward function\nFine-tune pre-trained model to maximize the reward using PPO\n\n\n\n\nDateset\n\n\nThe idea\nAlign with human preferences\n\nReward Model\n\nNotations\n\n\nReward Model\nOutline - Why take the negative and log - Why minus - Why take the sigmoid\n\\(\\operatorname{loss}(\\theta)=-\\frac{1}{\\left(\\begin{array}{c} K \\\\ 2 \\end{array}\\right)} E_{\\left(x, y_w, y_l\\right) \\sim D}\\left[\\log \\left(\\sigma\\left(r_\\theta\\left(x, y_w\\right)-r_\\theta\\left(x, y_l\\right)\\right)\\right)\\right]\\)\n\\(y_w\\), and \\(y_l\\): represent the output that prefered and non-prefereed by human labeller respectively\nThe goal of the reward model is make it algin with human preference as much as possible. So we want a loss score that indicate if the loss is high that mean the reward model isn’t doing good its job and vice versa. - \\(r_\\theta\\left(x, y_w\\right)\\) is the reward scalar of the reward model for prompt \\(x\\) and the summary \\(y_w\\) - \\(r_\\theta\\left(x, y_l\\right)\\)\n\n\nSigmoid Recap\n\nxs = torch.arange(-2, 2, step=0.1)\n\n\nplt.plot(xs.numpy(), torch.sigmoid(xs).numpy())\n\n\n\n\n\nInput: query and answer\nOutput: score (scalar)\n\nModels: GPT-J, GPT-Neo\nLoss: Cross-entropy. Which output labelers prefer\n\nDataset\npresent labelers with multiple responses to a prompt, and ask them to rank the responses\nhttps://huggingface.co/datasets/openai/webgpt_comparisons\n\n\nLoss function\n10 responses. compare pair\n\n\n\nSFT Model\n\nhttps://huggingface.co/Dahoas/gpt2-sft-static\nhttps://huggingface.co/Dahoas/gptneo-sft-static\nhttps://huggingface.co/Dahoas/gptj-sft-static\n\n\nimport torch\nimport torch.nn.functional as F\n\n\nx1 = torch.tensor(-0.1)\nx2 = torch.tensor(0.3)\nx3 = torch.tensor(0.5)\nx4 = torch.tensor(0.8)\n\n\ny = torch.tensor(0.3)\n\n\ny2 = torch.tensor(0.9)\n\n\nF.sigmoid(x)\n\ntensor(0.5498)\n\n\n\nF.sigmoid(y).log()\n\ntensor(-0.5544)\n\n\n\nF.sigmoid(y2).log()\n\ntensor(-0.3412)\n\n\n\nmaximize log likelihood = minimize negative log likelihood\n\n\nF.sigmoid(x1), F.sigmoid(x2), F.sigmoid(x3), F.sigmoid(x4)\n\n(tensor(0.4750), tensor(0.5744), tensor(0.6225), tensor(0.6900))\n\n\n\ntorch.tensor(1).log()\n\ntensor(0.)"
  },
  {
    "objectID": "posts/02_how_i_spent_my_time/analysis.html",
    "href": "posts/02_how_i_spent_my_time/analysis.html",
    "title": "What did i get done in last 9 months. How will i done differently?",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport plotly.express as px\n\nimport plotly.figure_factory as ff\nimport chart_studio.plotly as py\nimport plotly.figure_factory as ff\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport pandas as pd\nimport re\n\n\nData\n\n\nAnalysis\n\nstart_date, end_date = '2022-11-26', '2022-10-26'\n\n\nthis_month = filter_by_date(only_study, start_date, end_date)\n\n\n\nOverall\n\nI spent 2.256 hours on Project Foundation so far.\n\n\nfig = px.pie(task_by_hrs[:30], values=task_by_hrs[:30], names=task_by_hrs[:30].index, title='30th subject that i spent the most time')\nfig.show()\n\n\n                                                \n\n\n\n\nX\n\nfig = px.pie(this_month, values='duration_hours', names='Name', title='Number of hours spent per subject in November')\nfig.show()\n\n\n                                                \n\n\n\n\nBrain’s attention span is increable\n\nfocus_df\n\n\n\n\n\n  \n    \n      \n      Name\n      Category\n      Type\n      Duration\n      Pauses\n      Start date\n      End date\n      Notes\n      duration_hours\n      Month\n    \n  \n  \n    \n      1\n      Deep Learning\n      ML Engineering\n      fullFocus\n      59.400000\n      0.0\n      2022-11-26 09:30:51\n      2022-11-26 10:30:15\n      NaN\n      0.990000\n      November\n    \n    \n      3\n      Deep Learning\n      ML Engineering\n      fullFocus\n      60.850000\n      0.0\n      2022-11-26 07:37:41\n      2022-11-26 08:38:32\n      NaN\n      1.014167\n      November\n    \n    \n      5\n      Deep Learning\n      ML Engineering\n      fullFocus\n      53.733333\n      0.0\n      2022-11-26 06:32:44\n      2022-11-26 07:26:28\n      NaN\n      0.895556\n      November\n    \n    \n      7\n      Do anki\n      Anki\n      fullFocus\n      24.833333\n      0.0\n      2022-11-26 05:59:49\n      2022-11-26 06:24:39\n      NaN\n      0.413889\n      November\n    \n    \n      9\n      Do anki\n      Anki\n      fullFocus\n      26.350000\n      0.0\n      2022-11-26 04:41:10\n      2022-11-26 05:07:31\n      NaN\n      0.439167\n      November\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      5488\n      Learn Deepwork\n      Math\n      partialFocus\n      23.000000\n      0.0\n      2022-01-30 12:16:09\n      2022-01-30 12:39:09\n      NaN\n      0.383333\n      January\n    \n    \n      5489\n      Learn Deepwork\n      Math\n      fullFocus\n      25.000000\n      0.0\n      2022-01-30 11:49:26\n      2022-01-30 12:14:26\n      NaN\n      0.416667\n      January\n    \n    \n      5490\n      Learn Deepwork\n      Math\n      fullFocus\n      25.166667\n      0.0\n      2022-01-30 11:00:52\n      2022-01-30 11:26:02\n      NaN\n      0.419444\n      January\n    \n    \n      5492\n      Learn Deepwork\n      Math\n      partialFocus\n      25.000000\n      0.0\n      2022-01-30 10:29:19\n      2022-01-30 10:54:19\n      NaN\n      0.416667\n      January\n    \n    \n      5493\n      Learn Deepwork\n      Anki\n      partialFocus\n      31.000000\n      0.0\n      2022-01-30 09:26:33\n      2022-01-30 09:57:33\n      NaN\n      0.516667\n      January\n    \n  \n\n3099 rows × 10 columns\n\n\n\n\nfocus_df['Duration']\n\n1       59.400000\n3       60.850000\n5       53.733333\n7       24.833333\n9       26.350000\n          ...    \n5488    23.000000\n5489    25.000000\n5490    25.166667\n5492    25.000000\n5493    31.000000\nName: Duration, Length: 3099, dtype: float64\n\n\n\n\nX\n\nexclude_by_categories(session_data, 'Category', NONSTUDY_CATEGORY)\n\n\n\n\n\n  \n    \n      \n      Name\n      Category\n      Type\n      Duration\n      Pauses\n      Start date\n      End date\n      Notes\n      duration_hours\n      Month\n    \n  \n  \n    \n      0\n      Discovery\n      Anki\n      rest\n      24.681213\n      0.0\n      2022-11-26 10:30:16\n      2022-11-26 10:54:57\n      NaN\n      0.411354\n      November\n    \n    \n      1\n      Deep Learning\n      ML Engineering\n      fullFocus\n      59.400000\n      0.0\n      2022-11-26 09:30:51\n      2022-11-26 10:30:15\n      NaN\n      0.990000\n      November\n    \n    \n      2\n      Deep Learning\n      Anki\n      rest\n      52.291320\n      0.0\n      2022-11-26 08:38:34\n      2022-11-26 09:30:51\n      NaN\n      0.871522\n      November\n    \n    \n      3\n      Deep Learning\n      ML Engineering\n      fullFocus\n      60.850000\n      0.0\n      2022-11-26 07:37:41\n      2022-11-26 08:38:32\n      NaN\n      1.014167\n      November\n    \n    \n      4\n      Deep Learning\n      Anki\n      rest\n      11.185716\n      0.0\n      2022-11-26 07:26:30\n      2022-11-26 07:37:41\n      NaN\n      0.186429\n      November\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      5489\n      Learn Deepwork\n      Math\n      fullFocus\n      25.000000\n      0.0\n      2022-01-30 11:49:26\n      2022-01-30 12:14:26\n      NaN\n      0.416667\n      January\n    \n    \n      5490\n      Learn Deepwork\n      Math\n      fullFocus\n      25.166667\n      0.0\n      2022-01-30 11:00:52\n      2022-01-30 11:26:02\n      NaN\n      0.419444\n      January\n    \n    \n      5491\n      Learn Deepwork\n      Anki\n      rest\n      6.266667\n      0.0\n      2022-01-30 10:54:36\n      2022-01-30 11:00:52\n      NaN\n      0.104444\n      January\n    \n    \n      5492\n      Learn Deepwork\n      Math\n      partialFocus\n      25.000000\n      0.0\n      2022-01-30 10:29:19\n      2022-01-30 10:54:19\n      NaN\n      0.416667\n      January\n    \n    \n      5493\n      Learn Deepwork\n      Anki\n      partialFocus\n      31.000000\n      0.0\n      2022-01-30 09:26:33\n      2022-01-30 09:57:33\n      NaN\n      0.516667\n      January\n    \n  \n\n5202 rows × 10 columns\n\n\n\n\nsession_data.groupby('Name')['Duration'].sum()\n\nName\nAmerican Grammar                     996.368476\nAnki                                  50.083333\nArchitecture Patterns with Python    279.080462\nArtificial Neural Networks           231.726992\nAtomic Habits                        357.562572\n                                        ...    \nUS Government & Civics               386.571020\nVibrations and Waves                 379.754424\nWalk with FastAI                      88.340299\nWhat matters?                         74.926658\nmatplotlib                           208.119452\nName: Duration, Length: 129, dtype: float64\n\n\n\nsum_category = only_study.groupby('Category')['duration_hours'].sum()\n\n\nsum_category\n\nCategory\nAnki                    198.425835\nChemistry               178.653164\nGeneral Engineering      45.158056\nML Engineering          337.930572\nMath                    346.414453\nPhysics                 157.270505\nSoftware Engineering    178.353668\nName: duration_hours, dtype: float64\n\n\n\nfig = go.Figure(\n    data=[go.Bar(y=sum_category, x=sum_category.index)],\n    layout_title_text=\"Total hours i spent for each category\"\n)\nfig.show()\n\n\n                                                \n\n\n\n\nStudy Per day\n\nhrs_study_per_month\n\nStart date\n1      14.090556\n2     134.677222\n3     164.043333\n4     122.970123\n5     132.607397\n6      68.340570\n7     141.241003\n8     155.859104\n9     167.364444\n10    182.484722\n11    158.527778\nName: duration_hours, dtype: float64\n\n\n\nhrs_study_per_month = only_study.groupby(by=only_study['Start date'].dt.month)['duration_hours'].sum()\n\n\nhrs_study_per_month\n\nStart date\n1      14.090556\n2     134.677222\n3     164.043333\n4     122.970123\n5     132.607397\n6      68.340570\n7     141.241003\n8     155.859104\n9     167.364444\n10    182.484722\n11    158.527778\nName: duration_hours, dtype: float64\n\n\n\nfig = go.Figure(\n    data=[go.Bar(y=hrs_study_per_month, x=hrs_study_per_month.index)],\n    layout_title_text=\"Number of hours study per month\")\nfig.show()\n\n\n                                                \n\n\n\nhrs_study_per_day = only_study.groupby(by=only_study['Start date'].dt.date)['duration_hours'].sum()\n\n\nhrs_study_per_day\n\nStart date\n2022-01-30     3.863611\n2022-01-31    10.226944\n2022-02-01     5.173333\n2022-02-02     6.940278\n2022-02-03     5.514722\n                ...    \n2022-11-22     6.528333\n2022-11-23     6.266944\n2022-11-24     6.973056\n2022-11-25     5.725556\n2022-11-26     3.752778\nName: duration_hours, Length: 257, dtype: float64\n\n\n\nonly_study.groupby('Start date')['duration_hours'].sum()\n\nStart date\n2022-01-30 09:26:33    0.516667\n2022-01-30 10:29:19    0.416667\n2022-01-30 11:00:52    0.419444\n2022-01-30 11:49:26    0.416667\n2022-01-30 12:16:09    0.383333\n                         ...   \n2022-11-26 04:41:10    0.439167\n2022-11-26 05:59:49    0.413889\n2022-11-26 06:32:44    0.895556\n2022-11-26 07:37:41    1.014167\n2022-11-26 09:30:51    0.990000\nName: duration_hours, Length: 2704, dtype: float64\n\n\n\n\nSleep\n\nsleep_data\n\n\n\n\n\n  \n    \n      \n      Start\n      End\n      Sleep Quality\n      Regularity\n      Mood\n      Steps\n      Air Pressure (Pa)\n      City\n      Movements per hour\n      Time in bed (seconds)\n      Window start\n      Window stop\n      Did snore\n      Snore time\n      Weather temperature (°F)\n      Weather type\n      Notes\n    \n  \n  \n    \n      0\n      2021-08-12 23:42:44\n      2021-08-13 06:49:00\n      73%\n      —\n      NaN\n      0\n      NaN\n      NaN\n      48.685875\n      25575.832\n      NaN\n      NaN\n      False\n      0\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      2021-08-13 21:13:40\n      2021-08-14 08:37:23\n      93%\n      64%\n      NaN\n      0\n      NaN\n      NaN\n      44.422813\n      41023.318\n      NaN\n      NaN\n      False\n      0\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      2021-08-14 22:08:58\n      2021-08-15 04:48:43\n      45%\n      62%\n      NaN\n      0\n      NaN\n      NaN\n      94.705920\n      23984.457\n      NaN\n      NaN\n      False\n      0\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      2021-08-15 21:08:51\n      2021-08-16 05:10:01\n      76%\n      71%\n      NaN\n      0\n      NaN\n      NaN\n      69.665520\n      28869.459\n      NaN\n      NaN\n      False\n      0\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      2021-08-16 20:57:56\n      2021-08-17 05:03:17\n      85%\n      82%\n      OK\n      0\n      101.020004\n      Mỹ Đình\n      44.529137\n      29121.817\n      NaN\n      NaN\n      True\n      56\n      82.0\n      Cloudy\n      Good day\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      405\n      2022-11-21 17:25:19\n      2022-11-22 03:53:53\n      87%\n      98%\n      NaN\n      0\n      NaN\n      NaN\n      199.765470\n      37713.988\n      22-11-22 03:32:40\n      22-11-22 03:55:00\n      False\n      0\n      NaN\n      NaN\n      NaN\n    \n    \n      406\n      2022-11-22 17:22:38\n      2022-11-23 03:55:01\n      61%\n      98%\n      NaN\n      0\n      NaN\n      NaN\n      380.310180\n      37942.369\n      22-11-23 03:29:53\n      22-11-23 03:55:00\n      False\n      0\n      NaN\n      NaN\n      NaN\n    \n    \n      407\n      2022-11-23 17:20:53\n      2022-11-24 03:54:44\n      95%\n      98%\n      NaN\n      0\n      NaN\n      NaN\n      147.502240\n      38031.234\n      22-11-24 03:30:32\n      22-11-24 03:55:00\n      False\n      0\n      NaN\n      NaN\n      NaN\n    \n    \n      408\n      2022-11-24 17:46:11\n      2022-11-25 03:47:36\n      96%\n      98%\n      NaN\n      0\n      NaN\n      NaN\n      138.315950\n      36084.800\n      22-11-25 03:28:22\n      22-11-25 03:55:00\n      False\n      0\n      NaN\n      NaN\n      NaN\n    \n    \n      409\n      2022-11-25 17:19:03\n      2022-11-26 03:55:20\n      92%\n      96%\n      NaN\n      0\n      NaN\n      NaN\n      167.477500\n      38177.351\n      22-11-26 03:29:54\n      22-11-26 03:55:00\n      False\n      0\n      NaN\n      NaN\n      NaN\n    \n  \n\n410 rows × 17 columns\n\n\n\n\nSLEEP_COLUMNS = ['Start', 'End', 'Time in bed (seconds)']\n\n\nsleep_data = sleep_data.loc[:, sleep_data.columns.isin(SLEEP_COLUMNS)]\n\n\nsleep_data['Time in bed'] = sleep_data['Time in bed (seconds)'].apply(lambda x: x / 60 / 60)\n\n\nsleep_data\n\n\n\n\n\n  \n    \n      \n      Start\n      End\n      Time in bed (seconds)\n      Time in bed\n    \n  \n  \n    \n      0\n      2021-08-12 23:42:44\n      2021-08-13 06:49:00\n      25575.832\n      7.104398\n    \n    \n      1\n      2021-08-13 21:13:40\n      2021-08-14 08:37:23\n      41023.318\n      11.395366\n    \n    \n      2\n      2021-08-14 22:08:58\n      2021-08-15 04:48:43\n      23984.457\n      6.662349\n    \n    \n      3\n      2021-08-15 21:08:51\n      2021-08-16 05:10:01\n      28869.459\n      8.019294\n    \n    \n      4\n      2021-08-16 20:57:56\n      2021-08-17 05:03:17\n      29121.817\n      8.089394\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      405\n      2022-11-21 17:25:19\n      2022-11-22 03:53:53\n      37713.988\n      10.476108\n    \n    \n      406\n      2022-11-22 17:22:38\n      2022-11-23 03:55:01\n      37942.369\n      10.539547\n    \n    \n      407\n      2022-11-23 17:20:53\n      2022-11-24 03:54:44\n      38031.234\n      10.564232\n    \n    \n      408\n      2022-11-24 17:46:11\n      2022-11-25 03:47:36\n      36084.800\n      10.023556\n    \n    \n      409\n      2022-11-25 17:19:03\n      2022-11-26 03:55:20\n      38177.351\n      10.604820\n    \n  \n\n410 rows × 4 columns\n\n\n\n\nstudy_plot = px.line(hrs_study_per_day)\nstudy_plot.show()\n\n\n                                                \n\n\n\nsleep_plot = px.line(sleep_data, x='Start', y=\"Time in bed\")\nsleep_plot.show()\n\n\n                                                \n\n\n\n\nEfficiency\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/Mining-BTC-180.csv\")\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      Date\n      Number-transactions\n      Output-volume(BTC)\n      Market-price\n      Hash-rate\n      Cost-per-trans-USD\n      Mining-revenue-USD\n      Transaction-fees-BTC\n    \n  \n  \n    \n      0\n      0\n      2017-04-29 00:00:00\n      341319\n      4488916\n      3119179\n      4488916\n      9\n      3119179\n      256\n    \n    \n      1\n      1\n      2017-04-30 00:00:00\n      281489\n      3918072\n      2720216\n      3918072\n      10\n      2720216\n      199\n    \n    \n      2\n      2\n      2017-05-01 00:00:00\n      294786\n      3892124\n      2878278\n      3892124\n      10\n      2878278\n      228\n    \n    \n      3\n      3\n      2017-05-02 00:00:00\n      333161\n      4099704\n      3149553\n      4099704\n      10\n      3149553\n      273\n    \n    \n      4\n      4\n      2017-05-03 00:00:00\n      295149\n      3425069\n      2760373\n      3425069\n      10\n      2760373\n      247\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      174\n      174\n      2017-10-20 00:00:00\n      312409\n      10649227\n      13875337\n      10649227\n      46\n      13875337\n      162\n    \n    \n      175\n      175\n      2017-10-21 00:00:00\n      312257\n      9102412\n      12037733\n      9102412\n      40\n      12037733\n      168\n    \n    \n      176\n      176\n      2017-10-22 00:00:00\n      289131\n      10827706\n      14108349\n      10827706\n      50\n      14108349\n      155\n    \n    \n      177\n      177\n      2017-10-23 00:00:00\n      316096\n      10173284\n      13224118\n      10173284\n      43\n      13224118\n      186\n    \n    \n      178\n      178\n      2017-10-24 00:00:00\n      347220\n      9935312\n      12165126\n      9935312\n      36\n      12165126\n      200\n    \n  \n\n179 rows × 9 columns\n\n\n\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/Mining-BTC-180.csv\")\n\n\ntask_by_hrs\n\nName\nDeep Learning               217.083173\nDo anki                     175.992224\nClassical Mechanics          92.623889\nProbability Theory           72.220000\nChemchem                     69.201111\n                               ...    \nLearn Maple                   0.633333\nNumerical Computation         0.625833\nIntro to nanoengineering      0.609444\nJava                          0.558333\nAnki                          0.420278\nName: duration_hours, Length: 86, dtype: float64\n\n\n\ntask_by_hrs.index\n\nIndex(['Deep Learning', 'Do anki', 'Classical Mechanics', 'Probability Theory',\n       'Chemchem', 'General Chemistry', 'Learn Calculus 3',\n       'Learn Linear Algebra', 'Python OOP', 'Electricity and Magnetism',\n       'Learn Discrete Mathematics', 'Introduction to Machine Learning',\n       'Linear Algebra 3', 'Designing Machine Learning Systems',\n       'Learn A Level Chemistry', 'Differential Equations',\n       'Design Patterns in Python', 'Revision Linear Algebra',\n       'Relearn weak flashcards', 'Python Deep Dive', 'Pytorch', 'Statics',\n       'Robust Python', 'Thermodynamics I', 'Clean Code',\n       'General Chemistry 2', 'Probability Theory Revision', 'Matrix Calculus',\n       'Git', 'Making flashcard for linear algebra', 'Python Fundamental',\n       'Pytest', 'Pandas', 'SQL',\n       'Introduction to Materials Science & Engineering', 'SymPy',\n       'Learn Programming Abstraction', 'Revision Physics I',\n       'Introduction to Computational Thinking', 'Fullstack Dev',\n       'Find Deep Learning Resources', 'Vibrations and Waves', 'Julia',\n       'Revision Calculus', 'General Chemistry Prep', 'Data Engineering',\n       'Revision Mechanics', 'Bash', 'Architecture Patterns with Python',\n       'Data Structure and Algorithms', 'NumPy', 'Learn nbdev', 'Fastcore',\n       'Software Engineering', 'Modeling and Simulation',\n       'Artificial Neural Networks', 'Pymaterial', 'Github Actions',\n       'matplotlib', 'Docker', 'Mathematical Statistics',\n       'The Mathematics of Deep Learning', 'Learn Deepwork',\n       'Python Unit Test', 'Cognitive Bias', 'Scipy',\n       'Build elemagnet package', 'Elementary Statistics', 'Post-AGI',\n       'Deep Reinforcement Learning', 'Build general chemistry package',\n       'The Art of Approximation', 'Fluent Python',\n       'Machine Learning - Goodfellow', 'Python', 'Fix flashcards',\n       'Python Deep Dive 3', 'Walk with FastAI',\n       'Computational Linear Algebra', 'Learn Read Research Paper', 'FastAI',\n       'Learn Maple', 'Numerical Computation', 'Intro to nanoengineering',\n       'Java', 'Anki'],\n      dtype='object', name='Name')\n\n\n\n[task_by_hrs[k].tolist() for k in task_by_hrs.index]\n\n[217.08317342281342,\n 175.99222413076293,\n 92.62388888888889,\n 72.22,\n 69.20111111111112,\n 66.6078861591551,\n 62.104166666666664,\n 57.215833333333336,\n 36.67706770135297,\n 35.300227068861325,\n 31.10222222222222,\n 30.681007764273218,\n 28.779722222222222,\n 26.21361111111111,\n 24.87833333333333,\n 24.57472222222222,\n 22.7825,\n 22.430286514759064,\n 20.445,\n 19.705277777777777,\n 19.648888888888887,\n 15.244166666666667,\n 14.286666666666667,\n 14.004999999999999,\n 13.664722222222222,\n 13.171666666666667,\n 12.099722222222221,\n 11.358888888888888,\n 10.551666666666666,\n 9.969722222222222,\n 9.618544595241547,\n 8.732222222222223,\n 8.457222222222223,\n 8.411388888888888,\n 6.714444444444444,\n 6.501666666666666,\n 5.763888888888888,\n 5.7155555555555555,\n 5.133055555555555,\n 5.101666666666667,\n 4.989168686932988,\n 4.757777777777777,\n 4.6033333333333335,\n 4.375555555555556,\n 3.898611111111111,\n 3.8905555555555553,\n 3.6666666666666665,\n 3.6355555555555554,\n 3.5977777777777775,\n 3.528888888888889,\n 3.507222222222222,\n 3.4269444444444446,\n 3.373611111111111,\n 3.1916666666666664,\n 3.1680555555555556,\n 3.118888888888889,\n 2.8130555555555556,\n 2.6338888888888885,\n 2.631111111111111,\n 2.3405555555555555,\n 2.335,\n 2.1555555555555554,\n 2.1527777777777777,\n 2.132222222222222,\n 1.7850000000000001,\n 1.4430555555555555,\n 1.2013888888888888,\n 1.071388888888889,\n 1.0547222222222221,\n 0.9086111111111111,\n 0.8955555555555554,\n 0.8869444444444443,\n 0.8836111111111111,\n 0.8422222222222222,\n 0.8394444444444444,\n 0.8386111111111112,\n 0.8350000000000001,\n 0.8083333333333333,\n 0.7872222222222223,\n 0.7563888888888889,\n 0.6794444444444444,\n 0.6333333333333333,\n 0.6258333333333332,\n 0.6094444444444445,\n 0.5583333333333333,\n 0.42027777777777775]"
  },
  {
    "objectID": "posts/04_protein/latex.html",
    "href": "posts/04_protein/latex.html",
    "title": "xrs",
    "section": "",
    "text": "The loss function described in the text is used to train a Transformer-based neural network for protein generation. The goal of the model is to generate a sequence of amino acids, which together specify a protein, while being controlled by a set of descriptors known as control tags.\nThe variable \\(a\\) represents a sequence of amino acids and has length \\(n_a-1\\). The variable \\(c\\) represents the control tags, which are a set of descriptors such as protein family or source organism, and has length \\(n_c\\). The variable \\(x\\) is formed by prepending the control tag sequence to the amino acid sequence and has length \\(n=n_a+n_c\\). The probability over such a combined sequence is represented by \\(P(x)\\).\nThe language modeling problem is decomposed into a next-token prediction problem, where a token can either be an amino acid or a control tag. The neural network has parameters \\(\\theta\\) and is trained to minimize the negative log-likelihood over a dataset \\(D={x^1, \\ldots, x^{|D|}}\\). The loss function is calculated as follows:\n� ( � ) = − 1 ∣ � ∣ ∑ � = 1 ∣ � ∣ 1 � � ∑ � = 1 � � log ⁡ � � ( � � � ∣ � < � � ) L(D)=− ∣D∣ 1 ​\nk=1 ∑ ∣D∣ ​\nn k\n1 ​\ni=1 ∑ n k\n​ logp θ ​ (x i k ​ ∣x <i k ​ ) Once the neural network is trained, it can generate a new protein \\(\\underline{a}\\) of length \\(m_a\\) controlled by a control tag sequence \\(\\underline{c}\\) of length \\(m_c\\). The generation process starts by sequentially sampling its constituent tokens based on the learned parameters \\(\\theta\\). This is done by calculating the probabilities \\(p_\\theta\\left(a_1 \\mid \\underline{c}\\right), p_\\theta\\left(a_2 \\mid a_1, \\underline{c}\\right), \\ldots, p_\\theta\\left(a_j \\mid a_{<j}, \\underline{c}\\right)\\). The generation process continues until the model generates an ‘end of sequence’ token."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BLOG",
    "section": "",
    "text": "Current Projects\nI’m implementing RLHF (ChatGPT) and MuZero\nAbout me\nI am interested in particle physics, deep learning, neuroscience, material science, nanoscience, astrophysics and synthetic biology. I have designed my curriculum and study from 4:00 AM to 3:00 PM to get the information into my brain. Last month, I spent 200 hours studying consistently (24/7/365).\nI am streaming daily on Twitch: twitch.tv/xrsrke\nReach me? Just drop a message neuralink#7014 in discord, twitter/@xariusrke\n\n\n\n\n\n\n\n\n\n\n\nBreaking down RLHF\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n)\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nDaily updates from Project Foundation\n\n\n\n\n\n\n\nproject foundation\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2022\n\n\nxariusrke\n\n\n\n\n\n\n\n\nWhat did i get done in last 9 months. How will i done differently?\n\n\n\n\n\n\n\nproject foundation\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2022\n\n\nxariusrke\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "project_foundation.html",
    "href": "project_foundation.html",
    "title": "Project Foundation",
    "section": "",
    "text": "Currently I’m full-time working on Project Foundation.\nThe goal of project foundation is give me a foundation to give me a framework of thinking…\n\nPhysics\nDeep Learning\nMaterial Science\nNanoscience\nNeuroscience"
  }
]